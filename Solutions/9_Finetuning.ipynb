{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuClass": "premium"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "75x9mZNIbPAp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YUsYzWxfT8o-"
   },
   "outputs": [],
   "source": "!pip install --upgrade textblob 'keras-nlp' 'keras-preprocessing' gensim 'numpy<2' 'tensorflow-text==2.15.0' np-utils"
  },
  {
   "cell_type": "code",
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
    "embedding_dim = 100\n",
    "epochs=100\n",
    "batch_size = 100\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ],
   "metadata": {
    "id": "h3XUwgb0UBut"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f glove.6B.100d.txt ]; then\n",
    "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
    "fi"
   ],
   "metadata": {
    "id": "Tx3hZd6gUImG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!bash get_data.sh"
   ],
   "metadata": {
    "id": "PfyMUL8nXRYP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ],
   "metadata": {
    "id": "8ckCWLwTXVa0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"
   ],
   "metadata": {
    "id": "yV8JS-8harja"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ],
   "metadata": {
    "id": "B__Of4dUXXBU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ],
   "metadata": {
    "id": "fygJjt56Xhup"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')"
   ],
   "metadata": {
    "id": "pfkDahA5XnzL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ],
   "metadata": {
    "id": "bp6wXMtvXoJ4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=4, embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=True))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ],
   "metadata": {
    "id": "T1nRW7STX3sr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "CmuPaNl4YEc7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=10,  batch_size=250):\n",
    "    np.random.shuffle(np.array(corpus))\n",
    "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
    "    for batch in range(number_of_sentence_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = []\n",
    "        Y = []\n",
    "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []\n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = to_categorical(labels, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "        X = tf.constant(X)\n",
    "        Y = tf.constant(Y)\n",
    "        number_of_batches = len(X) // batch_size\n",
    "        for real_batch in range(number_of_batches):\n",
    "          lower_end = batch*batch_size\n",
    "          upper_end = (batch+1)*batch_size\n",
    "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
    "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
    "          yield (batch_X, batch_Y)"
   ],
   "metadata": {
    "id": "_433LAsvYHW-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        X, Y = generate_data(corpus=tokenized_corpus, vocab_size=vocab_size, batch_size=len(X))\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        model.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        for epoch in range(epochs):\n",
    "          index = 1\n",
    "          for x, y in generate_data(corpus=tokenized_corpus, vocab_size=vocab_size, batch_size=batch_size):\n",
    "              print(f'Training on Epoch/Iteration: {epoch+1}/{index}')\n",
    "              index += 1\n",
    "              history = model.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
    "              print(history)\n",
    "              if index > epochs:\n",
    "                    break"
   ],
   "metadata": {
    "id": "t8qf2WhiYwOJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fit_model()"
   ],
   "metadata": {
    "id": "UQz1WZF3atX3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fMuG5oPdau7j"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}