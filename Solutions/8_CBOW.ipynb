{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jbFcxFZhG5K"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade  textblob 'keras-nlp' np-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iklSJ4lqUQlT"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical, pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from textblob import TextBlob, Word\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
        "embedding_dim = 50\n",
        "epochs=2\n",
        "batch_size = 100\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  config = tf.compat.v1.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l13de14sclyD"
      },
      "outputs": [],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvRXU9EMVJMp"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAWXcLEieD4E"
      },
      "outputs": [],
      "source": [
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
        "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
        "X = yelp_best_worst.text\n",
        "y = yelp_best_worst.stars.map({1:0, 5:1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljgSnKkzeM4-"
      },
      "outputs": [],
      "source": [
        "yelp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUlTe1xsgi51"
      },
      "outputs": [],
      "source": [
        "TextBlob(X.values[0]).correct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfR6qIZZhIHd"
      },
      "outputs": [],
      "source": [
        "# Create corpus of sentences such that the sentence has more than 3 words\n",
        "corpus = [line for line in X.values if len(TextBlob(line).words)> 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B_z5Udki-_s"
      },
      "outputs": [],
      "source": [
        "corpus[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Z1eJZrhK7K",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `Tokenizer` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHtu75Kpi6XF"
      },
      "outputs": [],
      "source": [
        "tokenizer = TextVectorization()\n",
        "# Use the adapt method to fit the tokenizer\n",
        "tokenizer.adapt(corpus) # Fill\n",
        "\n",
        "print(f'Before the tokenizer: {corpus[:1]}')\n",
        "\n",
        "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs\n",
        "tokenized_corpus = tokenizer(corpus)\n",
        "\n",
        "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g44ICdUcj7ZL"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.get_vocabulary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTM2wqbzke5n"
      },
      "outputs": [],
      "source": [
        "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
        "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR97HVOqkoMI"
      },
      "outputs": [],
      "source": [
        "type(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvMp9eWsk2Z-"
      },
      "outputs": [],
      "source": [
        "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
        "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=15,  batch_size=250):\n",
        "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
        "    for batch in range(number_of_sentence_batches):\n",
        "        lower_end = batch*batch_size\n",
        "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
        "        mini_batch_size = upper_end - lower_end\n",
        "        maxlen = window_size*2\n",
        "        X = []\n",
        "        Y = []\n",
        "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
        "            L = len(words)\n",
        "            for index, word in enumerate(words):\n",
        "                contexts = []\n",
        "                labels   = []\n",
        "                s = index - window_size\n",
        "                e = index + window_size + 1\n",
        "\n",
        "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
        "                labels.append(word)\n",
        "\n",
        "                x = pad_sequences(contexts, maxlen=maxlen)\n",
        "                y = to_categorical(labels, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        X = tf.constant(X)\n",
        "        Y = tf.constant(Y)\n",
        "        number_of_batches = len(X) // batch_size\n",
        "        for real_batch in range(number_of_batches):\n",
        "          lower_end = batch*batch_size\n",
        "          upper_end = (batch+1)*batch_size\n",
        "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
        "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
        "          yield (batch_X, batch_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0dw_S7Kk6lW",
        "outputId": "ce29f09a-b1b5-4cc1-d432-3963bde9cf3d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Notice now in a sample how we construct X and y to predict words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCmSyCj8k6He"
      },
      "outputs": [],
      "source": [
        "iterable = generate_data(corpus=tokenized_corpus[:10], vocab_size=vocab_size, batch_size=10)\n",
        "sample_x, sample_y = next(iterable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjvrCdY5lkJk"
      },
      "outputs": [],
      "source": [
        "sample_y_numpy = sample_y.numpy()\n",
        "\n",
        "sample_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSdFKEvZa6qL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "np.where(sample_y_numpy == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cIBBOa4a6qM"
      },
      "source": [
        "Now comes the core part, defining the model. Keras provides a convenient Sequential model class to just `add` layers of any type and they will just work. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjtywT2Fa6qM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))  # Add an Embedding layer with input_dim vocab_size, output_dim to be embedding_dim, and the input_length to be twice our window\n",
        "cbow.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))  # Add a Lambda that takes a lambda function using the K.mean method to average the words. The output_shape should be (dim, ).\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))  # Add a classic Dense layer to just select with a softmax the best word\n",
        "# Compile the model with a loss and optimizer of your liking.\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "cbow.build(input_shape=(None, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIexLekTa6qM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cbow.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBNNSCeYa6qM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def fit_model():\n",
        "    if not BATCH:\n",
        "        X, Y = next(generate_data(corpus=tokenized_corpus[:20], vocab_size=vocab_size, batch_size=len(tokenized_corpus[:20])))\n",
        "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
        "        cbow.fit(X, Y, epochs = epochs, batch_size=batch_size)\n",
        "    else:\n",
        "        for epoch in range(epochs):\n",
        "            batch = 1\n",
        "            for x, y in generate_data(corpus=tokenized_corpus[:20], vocab_size=vocab_size, batch_size=batch_size):\n",
        "                history = cbow.train_on_batch(x, y, return_dict=True)\n",
        "                print(f'Epoch: {epoch}, Batch: {batch}, Loss: {history[\"loss\"]}')\n",
        "                batch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpEmERLpa6qM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "BATCH = False\n",
        "fit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6-Hhe_Wa6qN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "with open('./cbow_scratch_synonims.txt' ,'w') as f:\n",
        "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "    vectors = cbow.get_weights()[0]\n",
        "    for i, word in enumerate(tokenizer.get_vocabulary()):\n",
        "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
        "        f.write('{} {}\\n'.format(word, str_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QacFd_6Da6qN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./cbow_scratch_synonims.txt', binary=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsEzF8O1a6qN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "w2v.most_similar(positive=['gasoline'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDAAzHMka6qN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "w2v.most_similar(negative=['apple'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_wQsdlCa6qO",
        "pycharm": {
          "name": "#%%"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uCMkd6qbRxk"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}