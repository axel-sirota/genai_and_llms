{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIKJS_35hR8q"
   },
   "outputs": [],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnbTWB1khR5s"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'tensorflow-text==2.15.0' np_utils swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YkshUXDhR0Y"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, GRU, LSTM\n",
    "from keras import Model, Input\n",
    "import np_utils\n",
    "import swifter\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_nlp\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 200\n",
    "rnn_units = 256\n",
    "epochs=150\n",
    "buffer_size = 256\n",
    "corpus_size=25000\n",
    "test_corpus_size=5000\n",
    "# Batch size\n",
    "batch_size = 256\n",
    "min_count_words = 5\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCyoTGiHhRN-"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f train_corpus_descriptions_airbnb.csv ]; then\n",
    "  wget -O train_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/5rp7ibop99qyafo/train_corpus_descriptions_airbnb.csv?dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f test_corpus_descriptions_airbnb.csv ]; then\n",
    "    wget -O test_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/a29bbkg8hi4q4f4/test_corpus_descriptions_airbnb.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgyETfkigMHP"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1amH2OpgMHQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"./train_corpus_descriptions_airbnb.csv\"\n",
    "test_path = \"./test_corpus_descriptions_airbnb.csv\"\n",
    "# Read, then decode for py2 compat.\n",
    "airbnb_reviews = pd.read_csv(train_path, header=None, names=[\"review\"]).dropna().sample(n=corpus_size).reset_index(drop=True)\n",
    "test_airbnb_reviews = pd.read_csv(test_path, header=None, names=[\"review\"]).dropna().sample(n=test_corpus_size).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6n1DYQMmnfG"
   },
   "outputs": [],
   "source": [
    "airbnb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mJb_78qgMHR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the first review in text\n",
    "print(airbnb_reviews.iloc[0].review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atRfW5TgYU-x"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, should_join=True):\n",
    "    text = str(text)\n",
    "    text = ' '.join(str(word).lower() for word in textblob_tokenizer(text))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    if should_join:\n",
    "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "    else:\n",
    "      return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQlv5nWmnqw0"
   },
   "outputs": [],
   "source": [
    "def get_maximum_review_length(df):\n",
    "    maximum = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        candidate = len(preprocess_text(row.review, should_join=False))\n",
    "        if candidate > maximum:\n",
    "            maximum = candidate\n",
    "    return maximum\n",
    "\n",
    "\n",
    "maximum = get_maximum_review_length(airbnb_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMYUrVwFgMHR"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "freqs = defaultdict(int)\n",
    "\n",
    "# Construct the vocabulary, you are free to iterate the DF or use TF utilities.\n",
    "# What is key is that the vocab will be of words such that their frequency is above min_count_words!\n",
    "for ix, row in airbnb_reviews.iterrows():\n",
    "  for word in preprocess_text(row.review, should_join=False):\n",
    "    freqs[word] += 1\n",
    "    if freqs[word] > min_count_words:\n",
    "      vocab.add(word)\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9e_y27xgMHS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{len(vocab)} unique words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq5wMDSugMHT"
   },
   "outputs": [],
   "source": [
    "ids_from_words = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)  # Construct the ids from words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czQvLYzDgMHT"
   },
   "outputs": [],
   "source": [
    "words_from_ids = preprocessing.StringLookup(invert=True, vocabulary=ids_from_words.get_vocabulary(), mask_token=None)  # Construct the ids from words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdEueo3ngMHU"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(words_from_ids(ids), axis=-1, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knNacRyjgMHU"
   },
   "outputs": [],
   "source": [
    "ids = ids_from_words(preprocess_text('Only you can prevent forest fires', should_join=False))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVm2DhrSt9bs"
   },
   "outputs": [],
   "source": [
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOM7u4Jx9KIp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ids_from_text(text):\n",
    "  return ids_from_words(preprocess_text(text, should_join=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxAVB1LZ9KIp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ids_from_text('Only you can prevent forest fires')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUmUlypjAGwp"
   },
   "outputs": [],
   "source": [
    "# Not computationally cool solution\n",
    "\n",
    "# def get_ids_tensor(df):\n",
    "#   all_ids = tf.constant(np.zeros((1, maximum)), dtype='int64')\n",
    "#   for review in df.review:\n",
    "#       review = preprocess_text(review, should_join=False) # Preprocess the review (should it output text or a list?)\n",
    "#       value = ids_from_words(review[0: maximum])\n",
    "#       if len(review) < maximum:\n",
    "#         value = tf.concat([value, tf.zeros((maximum-len(review)), dtype='int64')], axis=0)  # if the review has fewer words than maximum pad with zeroes on the right\n",
    "\n",
    "#       # value pad_sequences([value], maxlen=maximum, dtype='int64', padding='post', value=0)\n",
    "\n",
    "\n",
    "#       value = tf.reshape(value, [1, maximum])\n",
    "#       output = tf.concat([all_ids, value], axis=0) # concatenate\n",
    "#       all_ids = tf.reshape(output, [-1, maximum])  # reshape\n",
    "#   return all_ids[1:]\n",
    "\n",
    "# all_ids = get_ids_tensor(df=airbnb_reviews.head())\n",
    "# print(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6s18du1gMHW"
   },
   "outputs": [],
   "source": [
    "# Cool solution\n",
    "\n",
    "import swifter\n",
    "def pad_sequence_of_tokens(x, maxlen, unk_token='[UNK]'):\n",
    "  if len(x)<maxlen:\n",
    "    x.extend([unk_token]*(maxlen-len(x)))\n",
    "  return x\n",
    "\n",
    "def get_ids_tensor(df):\n",
    "\n",
    "  df['processed'] = df.review.swifter.apply(lambda x: pad_sequence_of_tokens(preprocess_text(x, should_join=False), maxlen=maximum))\n",
    "  processed = df.processed\n",
    "  return tf.squeeze(tf.constant(pad_sequences(ids_from_words(processed.tolist()), maxlen=maximum, padding='post'), dtype='int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YYp6t2KgMHW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_ids = get_ids_tensor(df=airbnb_reviews)\n",
    "print(all_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzym60TTgMHW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_all_ids = get_ids_tensor(df=test_airbnb_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYRoOShmgMHX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Prepare the dataset\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # Create tensorflow Dataset object from all_ids tensor\n",
    "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_all_ids) # Do likewise for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GR56NhN2-iL"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp6f_sr-gMHX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ids_dataset.map(split_input_target)\n",
    "test_dataset = test_ids_dataset.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ0cvE_YgMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7rn27Gd9KIr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size=batch_size, drop_remainder=True)\n",
    ")\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .batch(batch_size=batch_size, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eatMUpcm9KIs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dcNCEWogMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RentalGenerator(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units, seq_length):\n",
    "    super().__init__(self)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.seq_length = seq_length\n",
    "    self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length) # Create Embedding layer, which should be its shape? If you want test out training a word2vec and set it here to fine-tune\n",
    "    self.lstm = LSTM(units=rnn_units, activation='tanh', return_sequences=True, return_state=True) # Create a LSTM layer with tanh activation to ensure you can use cuDNN libraries. You should return both sequences and states\n",
    "    self.dense = Dense(vocab_size) # Final Dense layer as always\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.lstm.get_initial_state(x)\n",
    "    output = self.lstm(x, initial_state = states, training=training)\n",
    "    x = output[0]\n",
    "    states = output[1:]\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "  def build_graph(self):\n",
    "    x = Input(shape=(self.seq_length, ))   # doesn't consider the batch\n",
    "    Model(inputs=x, outputs=self.call(x))\n",
    "    self.build((None, self.seq_length, ))   # takes into consideration the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzX4F9z-gMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = RentalGenerator(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_words.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    seq_length=maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYw8nf2agMHZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KQWnLby8G5t"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WQEZfLg8Hxd"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy0-768EgMHa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv2UPlTWgMHa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras_core.losses import SparseCategoricalCrossentropy\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, name='perplexity', mask_token_id =0)\n",
    "model.compile(optimizer='adam', metrics=[perplexity], loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "# Compile and train the model such that it early stops if the perplexity in the val set does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVWZTnQegMHa"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_perplexity', patience=5)\n",
    "history = model.fit(dataset, epochs=epochs, workers=5, validation_data=test_dataset, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM7WYkdH9KIv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], val_metric=history.history['val_loss'], metric_name=\"Loss\", title=\"Loss\", ylim=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4wZLjZQ9KIw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(history.history['perplexity'], val_metric=history.history['val_perplexity'], metric_name=\"perplexity\", title=\"perplexity\", ylim=2000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBG18sCw9KIw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, word_from_ids, ids_from_words, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.word_from_ids = word_from_ids\n",
    "    self.ids_from_words = ids_from_words\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_words.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function(reduce_retracing=True)\n",
    "  def expand_dims_if_neccesary(self, input):\n",
    "    if len(input.shape) < 3:\n",
    "      input = tf.expand_dims(input, axis=0)\n",
    "    return input\n",
    "\n",
    "  @tf.function(reduce_retracing=True)\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_words = preprocess_text(inputs, should_join=False)\n",
    "    input_ids = self.expand_dims_if_neccesary(self.ids_from_words(input_words))\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_word = self.word_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_word, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TACqOESm9KIw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, words_from_ids, ids_from_words)\n",
    "start = time.time()\n",
    "states = None\n",
    "description = tf.constant(['Midtown Sunny 2-Bedroom'])\n",
    "# result = [description]\n",
    "\n",
    "for n in range(100):\n",
    "  next_word, states = one_step_model.generate_one_step(description, states=states)\n",
    "  description = tf.concat([description, next_word], axis=0)\n",
    "\n",
    "\n",
    "result = tf.strings.join(description, separator=\" \")\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmNCGyCY9KIx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'lstm_rental_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvd9TcbS9KIx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_i1ZF4Y9KIx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAKdnRq-9KIx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words_from_ids(tf.math.argmax(tf.math.softmax(example_batch_predictions[0]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDj6agGJCZdh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}