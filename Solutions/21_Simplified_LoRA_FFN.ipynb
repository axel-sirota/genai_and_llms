{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMgh6GdbrIZzeixko65khXQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Simplified LoRA adaptation of FFN"],"metadata":{"id":"LHd2B-47wlEn"}},{"cell_type":"markdown","source":["We will show how to do LoRA on a simple FFN by first pre-training it on Fashion MNIST and then finetune it on MNIST. As those datasets don't have a ton to do the performance will be quite bad, but we seek to show how to do PEFT in general regardless of the model"],"metadata":{"id":"pqoxVXkCwrRu"}},{"cell_type":"markdown","source":["## Pre-Training"],"metadata":{"id":"lXVXbviLxE6H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er9WwABqnXZo"},"outputs":[],"source":["pip install datasets"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# Load Fashion MNIST dataset\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Normalize the images\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Define the more complex model\n","model = keras.Sequential([\n","    Flatten(input_shape=(28, 28)),\n","\n","    Dense(1024, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(512, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(256, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(128, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(64, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(10, activation='softmax')  # 10 classes in Fashion MNIST\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n"],"metadata":{"id":"VR_yyLKGntCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"9yHTYWEtoRCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","model.fit(train_images, train_labels, epochs=15, validation_split=0.2)"],"metadata":{"id":"IMWnJHcQoSK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lora-Adaptation"],"metadata":{"id":"yYJjIaefwkJG"}},{"cell_type":"markdown","source":["Load the new dataset"],"metadata":{"id":"Oix8xfCYoEIa"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","\n","# Load MNIST dataset\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","# Normalize the images\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Reshape images for the model\n","train_images = train_images.reshape((-1, 28, 28, 1))\n","test_images = test_images.reshape((-1, 28, 28, 1))\n"],"metadata":{"id":"k9LOAy-fn_33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LoraLayer(keras.layers.Layer):\n","    def __init__(\n","        self,\n","        original_layer,\n","        rank=8,\n","        num_heads =1,\n","        dim = 1,\n","        trainable=False,\n","        **kwargs,\n","    ):\n","        # We want to keep the name of this layer the same as the original\n","        # dense layer.\n","        original_layer_config = original_layer.get_config()\n","        name = original_layer_config[\"name\"]\n","\n","        kwargs.pop(\"name\", None)\n","\n","        super().__init__(name=name, trainable=trainable, **kwargs)\n","\n","        self.rank = rank\n","\n","\n","        # Layers.\n","\n","        # Original dense layer.\n","        self.original_layer = original_layer\n","        # No matter whether we are training the model or are in inference mode,\n","        # this layer should be frozen.\n","        self.original_layer.trainable = False\n","\n","        # LoRA dense layers.\n","        self.A = keras.layers.Dense(\n","            units=rank,\n","            use_bias=False,\n","            trainable=trainable,\n","            name=f\"lora_A\",\n","        )\n","\n","        self.B = keras.layers.Dense(\n","            units=dim,\n","            use_bias=False,\n","            trainable=trainable,\n","            name=f\"lora_B\",\n","        )\n","\n","    def call(self, inputs):\n","        original_output = self.original_layer(inputs)\n","        if self.trainable:\n","            # If we are fine-tuning the model, we will add LoRA layers' output\n","            # to the original layer's output.\n","            lora_output = self.B(self.A(inputs))\n","            return original_output + lora_output\n","\n","        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n","        # the original layer's weights - more on this in the text generation\n","        # section!\n","        return original_output"],"metadata":{"id":"MJoRRBCYntJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will randomly change some Dense layers into Lora Adapted layers"],"metadata":{"id":"Vznpe0o2xOA_"}},{"cell_type":"code","source":["import random\n","# Define a function to replace dense layers with LoraLayer\n","def replace_with_lora(model):\n","    new_model = keras.Sequential()\n","    for layer in model.layers:\n","        if isinstance(layer, Dense) and random.random() > 0.5:\n","            new_model.add(LoraLayer(original_layer=layer, rank=4, dim=layer.units, trainable=True))\n","        else:\n","            new_model.add(layer)\n","    return new_model\n","\n","# Replace layers in the model\n","lora_model = replace_with_lora(model)\n","\n","lora_model.build(input_shape=(None, 28, 28, 1))\n","\n","# Compile the model\n","lora_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","                   loss='sparse_categorical_crossentropy',\n","                   metrics=['accuracy'])\n"],"metadata":{"id":"EgUwvc3-oLcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_model.summary()"],"metadata":{"id":"ssb8zSUxojvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the non-trainable parameters"],"metadata":{"id":"9V3rewF0xVXa"}},{"cell_type":"code","source":["# Fine-tune the model\n","lora_model.fit(train_images, train_labels, epochs=10, validation_split=0.2)"],"metadata":{"id":"BFHihSHYom3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As mentioned, performance sucks, but the important thing is that we finetuned only the LoraLayers"],"metadata":{"id":"fdv1pMzXxYwW"}},{"cell_type":"code","source":[],"metadata":{"id":"EkJqOtFusXQH"},"execution_count":null,"outputs":[]}]}