{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEqSAWGgNeRx"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IY2pTPBPir1"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'tensorflow-text==2.15.0'' np_utils swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gZklVraP2vn"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 300\n",
    "epochs=100\n",
    "batch_size = 250\n",
    "corpus_size=25000\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0DZp9-YP8Fa"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f news.csv ]; then\n",
    "  wget -O news.csv https://www.dropbox.com/s/352x7xzivf60zgc/news.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Arc-2X3AS2dI"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIsjpoQMS5tT"
   },
   "outputs": [],
   "source": [
    "path = './news.csv'\n",
    "news_pre = pd.read_csv(path, header=0).sample(n=corpus_size).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e48igNipS-7m"
   },
   "outputs": [],
   "source": [
    "news_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmuzxBe0S-45"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text, should_join=True):\n",
    "    # Use the tokenizer to tokenize into words, lowercase them, remove punctuation, and finally use gensim.utils.simple_preprocess(text)\n",
    "    text = ' '.join(gensim.utils.tokenize(text, lowercase=True))\n",
    "    text = re.sub(r\"[.,!?]\", r\" \", text)\n",
    "    if should_join:\n",
    "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "    else:\n",
    "      return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BI4j9GohS-1k"
   },
   "outputs": [],
   "source": [
    "preprocess_text('This is the best night of my life! Is it? Well, maybe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS0pq9sbO7hy"
   },
   "outputs": [],
   "source": [
    "import swifter\n",
    "news = news_pre.title.swifter.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7w17pWkRf3LI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "news.to_csv('news_pre.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Fsl1VwnVJoz"
   },
   "outputs": [],
   "source": [
    "!head -n 5 news_pre.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cFi0e_sVJmh"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = 'news_pre.csv'\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield preprocess_text(line, should_join=False)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=MyCorpus(), vector_size=embedding_dim)\n",
    "# Get a word2vec model using gensim.models and passing the sentences using MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HspIJVUBVJkF"
   },
   "outputs": [],
   "source": [
    "wv_model = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z5V4riHX8lf"
   },
   "outputs": [],
   "source": [
    "weights = tf.constant(wv_model.vectors)  # Get the weights of the model (the embedding) and convert to tensor. Hint: Check word2vec_model.wv\n",
    "vocab_size = len(wv_model.index_to_key)  # get vocab size from index_to_key in word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSvBEG4_aV-3"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWpiksEJYWyr"
   },
   "outputs": [],
   "source": [
    "news_preprocessed = pd.DataFrame()\n",
    "news_preprocessed['label'] = news_pre.category.map({'Business': 0, 'Sports': 1, 'Sci/Tech': 2, 'World': 3})\n",
    "news_preprocessed['title'] = news\n",
    "news_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzrIQ4G0VJhh"
   },
   "outputs": [],
   "source": [
    "def get_maximum_review_length(df):\n",
    "    maximum = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        candidate = len(textblob_tokenizer(row.title))\n",
    "        if candidate > maximum:\n",
    "            maximum = candidate\n",
    "    return maximum\n",
    "\n",
    "\n",
    "maximum = get_maximum_review_length(news_preprocessed)   # Since 2 titles may have different number of words, we have to find the max length and fill with 0s if a title is shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T-hPDiIVJe9"
   },
   "outputs": [],
   "source": [
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCQiCKLSVJcg"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(news_preprocessed), maximum))   # Here we do what we said above\n",
    "# Iterate through the news df and for every word, if it exists in the word2vec model, put into X for that review and that word the index of the embedding (check index_to_key)\n",
    "# HINT: to iterate through a column of a pandas dataframe you do:\n",
    "\n",
    "# for index, value in df.iterrows():\n",
    "#      #do something\n",
    "\n",
    "for index, row in news_preprocessed.iterrows():\n",
    "  word_ix = 0\n",
    "  for word in textblob_tokenizer(row.title):\n",
    "    token = vocab_size+1\n",
    "    if word in wv_model.key_to_index:\n",
    "      token = wv_model.key_to_index[word]\n",
    "    X[index, word_ix] = token\n",
    "    word_ix += 1\n",
    "\n",
    "# FILL\n",
    "y = news_preprocessed.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDKdanUVa4GT"
   },
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzGMhna1UrTe"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = tf.constant(X_train)\n",
    "X_test = tf.constant(X_test)\n",
    "# Convert y_train and y_test from an array of values between 0-3 to a one hot matrix tensor\n",
    "y_train = tf.one_hot(y_train, 4)\n",
    "y_test = tf.one_hot(y_test, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3N8INqJczF0"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], input_length=maximum, trainable=False, embeddings_initializer=Constant(weights)))  # Add an Embedding layer with weights being the rweights variable and trainable as False. The embedding dimension should be embedding_dim\n",
    "model.add(Dense(100, activation='relu'))  # Add a couple of Dense Layers with RELU or leaky_relu activations. You may add Batch Norm if you want too\n",
    "model.add(Dense(50, activation='tanh'))  # Add a couple of Dense Layers with RELU or leaky_relu activations. You may add Batch Norm if you want too\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(None, embedding_dim,)))  # Average out the words of the sentence. The expected out is (N, D) where N is number f samples in batch and D is embedding dimension\n",
    "model.add(Dense(25, activation=leaky_relu))\n",
    "model.add(Dense(4, activation='softmax'))  # Add final Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcqS9NGOLYVP"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Compile the model. Think what is the best loss to use\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av6wncrGVuHH"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=y_train, epochs=epochs, batch_size=batch_size, workers=5, validation_data=(X_test, y_test)) # Fit the model, use the callback above to do EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLrVc0WaR0_W"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV37pfP3VZwM"
   },
   "outputs": [],
   "source": [
    "plot_metrics(history.history['accuracy'], history.history['val_accuracy'], \"Accuracy\", \"Accuracy\", ylim=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1fTU2I2V5__"
   },
   "outputs": [],
   "source": [
    "# Test with the following two sentences:\n",
    "# - 'supercomputer will put workers jobless soon'\n",
    "# - 'patriots goes winning super bowl'\n",
    "\n",
    "x_val = np.zeros((2, maximum))\n",
    "for index, row in enumerate(['supercomputer will put workers jobless soon', 'patriots goes winning super bowl']):\n",
    "    word_ix = 0\n",
    "    for word in textblob_tokenizer(row):\n",
    "      token = vocab_size+1\n",
    "      if word in wv_model.key_to_index:\n",
    "        token = wv_model.key_to_index[word]\n",
    "      x_val[index, word_ix] = token\n",
    "      word_ix += 1\n",
    "y_val = tf.one_hot([0,1], depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG3ZkYp8f3LO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_val = tf.constant(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7l1Wy2_f3LO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C9WKGqPf3LP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YlIsYKyf3LP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh9OwiiSf3LP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKxGpvR9f3LP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}