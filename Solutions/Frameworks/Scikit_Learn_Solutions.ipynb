{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b508698a",
   "metadata": {
    "id": "b508698a"
   },
   "source": [
    "\n",
    "# Scikit-Learn Exercises for Basic Data and Model Manipulation\n",
    "\n",
    "This notebook provides a set of exercises designed to familiarize you with the basics of handling datasets, models,\n",
    "train-test splits, and cross-validation in scikit-learn, essential for machine learning tasks.\n",
    "\n",
    "## Exercises Overview\n",
    "1. Loading and Exploring Datasets\n",
    "2. Preprocessing Data\n",
    "3. Creating and Evaluating Models\n",
    "4. Train-Test Split\n",
    "5. Cross-Validation\n",
    "6. Hyperparameter Tuning\n",
    "\n",
    "Each section will include a brief explanation followed by practical exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b966aa",
   "metadata": {
    "id": "54b966aa"
   },
   "source": [
    "\n",
    "## 1. Loading and Exploring Datasets\n",
    "\n",
    "**Objective**: Learn how to load datasets from scikit-learn and explore their features.\n",
    "\n",
    "**Exercises**:\n",
    "1. Load the Iris dataset and display its description.\n",
    "2. Find the number of samples and features in the dataset.\n",
    "3. Visualize the distribution of target classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-0wIEQXOkIV2",
   "metadata": {
    "id": "-0wIEQXOkIV2"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Exercise 1: Loading and Exploring Datasets\n",
    "\n",
    "# 1.1 Load the Iris dataset and display its description.\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)\n",
    "\n",
    "# 1.2 Find the number of samples and features in the dataset.\n",
    "num_samples, num_features = iris.data.shape\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# 1.3 Visualize the distribution of target classes.\n",
    "# Converting to DataFrame for easier plotting\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "# Plotting the distribution of target classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='target', data=iris_df)\n",
    "plt.title('Distribution of Target Classes in Iris Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730de6e3",
   "metadata": {
    "id": "730de6e3"
   },
   "source": [
    "\n",
    "## 2. Preprocessing Data\n",
    "\n",
    "**Objective**: Understand basic data preprocessing techniques.\n",
    "\n",
    "**Exercises**:\n",
    "1. Standardize the features of a dataset.\n",
    "2. Perform a principal component analysis (PCA) to reduce the dataset to two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RV_h93p4kQh3",
   "metadata": {
    "id": "RV_h93p4kQh3"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exercise 2: Preprocessing Data\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# 2.1 Standardize the features of a dataset.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2.2 Perform a principal component analysis (PCA) to reduce the dataset to two dimensions.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualization (Optional)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap='viridis')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.title('2D PCA of Iris Dataset')\n",
    "plt.colorbar(label='Target Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53589296",
   "metadata": {
    "id": "53589296"
   },
   "source": [
    "\n",
    "## 3. Creating and Evaluating Models\n",
    "\n",
    "**Objective**: Learn to create models and evaluate their performance.\n",
    "\n",
    "**Exercises**:\n",
    "1. Create a logistic regression model and fit it to a dataset.\n",
    "2. Evaluate the model's accuracy using a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DP_3m7sPkZOc",
   "metadata": {
    "id": "DP_3m7sPkZOc"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Exercise 3: Creating and Evaluating Models\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3.1 Create a logistic regression model and fit it to the dataset.\n",
    "log_reg_model = LogisticRegression(max_iter=200)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# 3.2 Evaluate the model's accuracy using a test set.\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Outputting the accuracy\n",
    "print(\"Model Accuracy:\", model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0b62d",
   "metadata": {
    "id": "cbc0b62d"
   },
   "source": [
    "\n",
    "## 4. Train-Test Split\n",
    "\n",
    "**Objective**: Perform train-test splits to prepare data for model training.\n",
    "\n",
    "**Exercises**:\n",
    "1. Split a dataset into training and testing sets.\n",
    "2. Ensure the split has stratified sampling based on the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AI1yrlQqkiGk",
   "metadata": {
    "id": "AI1yrlQqkiGk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 4: Train-Test Split\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 4.1 Split a dataset into training and testing sets.\n",
    "# Splitting the dataset into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 4.2 Ensure the split has stratified sampling based on the target variable.\n",
    "# Performing a stratified split to maintain the distribution of the target variable\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Checking the distribution of target classes in the original and stratified splits\n",
    "original_dist = pd.Series(y).value_counts(normalize=True)\n",
    "train_dist = pd.Series(y_train_strat).value_counts(normalize=True)\n",
    "test_dist = pd.Series(y_test_strat).value_counts(normalize=True)\n",
    "\n",
    "# Outputting the distributions\n",
    "original_dist, train_dist, test_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99505db6",
   "metadata": {
    "id": "99505db6"
   },
   "source": [
    "\n",
    "## 5. Cross-Validation\n",
    "\n",
    "**Objective**: Understand and implement cross-validation.\n",
    "\n",
    "**Exercises**:\n",
    "1. Perform k-fold cross-validation on a model.\n",
    "2. Compare the average performance across different folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stzP5VpFkpfo",
   "metadata": {
    "id": "stzP5VpFkpfo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Exercise 5: Cross-Validation\n",
    "\n",
    "# 5.1 Perform k-fold cross-validation on a model.\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Setting up k-fold cross-validation (e.g., 5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=kf)\n",
    "\n",
    "# 5.2 Compare the average performance across different folds.\n",
    "average_performance = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f32cc",
   "metadata": {
    "id": "132f32cc"
   },
   "source": [
    "\n",
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "**Objective**: Learn how to tune model hyperparameters for better performance.\n",
    "\n",
    "**Exercises**:\n",
    "1. Use GridSearchCV to find the best hyperparameters for a model.\n",
    "2. Analyze the results of the grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vwYTT9U_kyeh",
   "metadata": {
    "id": "vwYTT9U_kyeh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Exercise 6: Hyperparameter Tuning\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6.1 Use GridSearchCV to find the best hyperparameters for a RandomForestClassifier.\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6.2 Analyze the results of the grid search.\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Best score\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Results\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
