{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO4He6AuvCvD"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'tensorflow-text==2.15.0' np_utils swifter"
   ],
   "metadata": {
    "id": "qT84LO_ryWGC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, LSTM, TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from keras import Model, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import np_utils\n",
    "import swifter\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_nlp\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "rnn_units = 128\n",
    "epochs=100\n",
    "buffer_size = 256\n",
    "max_len = 50\n",
    "# Batch size\n",
    "batch_size = 256\n",
    "min_count_words = 3\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ],
   "metadata": {
    "id": "6h9-CflyyWIf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f ner_dataset.csv ]; then\n",
    "  wget -O ner_dataset.csv https://www.dropbox.com/s/mbfv0x988mdj89h/ner_dataset.csv?dl=0\n",
    "fi\n"
   ],
   "metadata": {
    "id": "0oENn5_JyWLG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!bash get_data.sh"
   ],
   "metadata": {
    "id": "CkxCOJz_yWNu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data= pd.read_csv(\"./ner_dataset.csv\",encoding=\"latin1\")\n",
    "data = data.fillna(method='ffill')\n",
    "data.head()"
   ],
   "metadata": {
    "id": "18V5m92oyWQK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Unique Words in corpus:\",data['Word'].nunique())\n",
    "print(\"Unique Tag in corpus:\",data['Tag'].nunique())"
   ],
   "metadata": {
    "id": "8RzRPJPTyWS2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "words = list(set(data['Word'].values))\n",
    "words.append(\"ENDPAD\")\n",
    "num_words = len(words)\n",
    "tags = list(set(data['Tag'].values))\n",
    "num_tags = len(tags)"
   ],
   "metadata": {
    "id": "oYF9u8Dp13YW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SentenceGetter(object):\n",
    "  def __init__(self,data):\n",
    "    self.n_sent = 1 #counter\n",
    "    self.data = data\n",
    "    agg_func = lambda s:[(w,p,t) for w,p,t in zip(s['Word'].tolist(),s['POS'].tolist(),s['Tag'].tolist())]\n",
    "    self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "    self.sentences = [s for s in self.grouped]\n",
    "\n",
    "\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ],
   "metadata": {
    "id": "1XAv4P3G1-7D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentences[0]"
   ],
   "metadata": {
    "id": "9tP7TzZ71-9d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word2idx =  {w : i+1 for i,w in enumerate(words)}\n",
    "tag2idx  =  {t : i for i,t in enumerate(tags)}"
   ],
   "metadata": {
    "id": "R4IPXmOQ1_Ci"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = [[word2idx[w[0]] for w in s]for s in sentences]\n",
    "X = pad_sequences(maxlen = max_len , sequences =X, padding='post', value =num_words-1)\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s]for s in sentences]\n",
    "y = pad_sequences(maxlen = max_len , sequences =y, padding='post', value =tag2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=num_tags) for i in y]\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.1, random_state=1)\n"
   ],
   "metadata": {
    "id": "0ox9i1Zc1_FN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = (max_len,)))\n",
    "model.add(Embedding(input_dim=num_words,output_dim=max_len,input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(Bidirectional(LSTM(units=100, activation = 'tanh', recurrent_activation = 'sigmoid', return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(num_tags,activation='softmax')))"
   ],
   "metadata": {
    "id": "z1JGVp1x30AY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=[tf.keras.metrics.KLDivergence(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "877hAFmB4lJK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "early_stopping = EarlyStopping(monitor='kullback_leibler_divergence',patience=5, verbose=0, mode='min',restore_best_weights=False)\n",
    "history = model.fit(x_train, np.array(y_train), validation_split = 0.2, batch_size = batch_size, epochs = epochs, verbose = 1, callbacks=[early_stopping])"
   ],
   "metadata": {
    "id": "D3-ZFBRX4soC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.evaluate(x_test, np.array(y_test))"
   ],
   "metadata": {
    "id": "QF6I4FYU5LKf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i = np.random.randint(0, x_test.shape[0])\n",
    "p = model.predict(np.array([x_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "y_true = np.argmax(np.array(y_test), axis=-1)[i]\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\",\"True\",\"Pred\"))\n",
    "print(\"-\"*30)\n",
    "for w,true,pred in zip(x_test[i], y_true, p[0]):\n",
    "  print(\"{:15}{:5}\\t{}\".format(words[w-1], tags[true],tags[pred]))\n"
   ],
   "metadata": {
    "id": "DFgXTGOE5Mia"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "DCyQIGqG6alm"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}